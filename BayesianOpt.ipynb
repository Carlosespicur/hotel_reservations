{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdAhwJlcoW5q"
      },
      "source": [
        "#Creación de una red neuronal artificial para predecir si el cliente cancelará la reserva del hotel (Clasisficación binaria)\n",
        "### Keras\n",
        "Javier López González\n",
        "\n",
        "Alejandro Ruiz\n",
        "\n",
        "Carlos Espinoza\n",
        "\n",
        "Miguel Barragan Rodriguez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mc93g0NKaoW"
      },
      "source": [
        "## Import libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NszkOjZ6OGXL",
        "outputId": "8912e1de-5e88-4b60-cbce-7c3edffc3580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.11.17)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.6 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kUkW4JNxoW5s"
      },
      "outputs": [],
      "source": [
        "#para una barra de progreso durante el entrenamiento\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "\n",
        "# Tensorflow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import keras_tuner\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Helper libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwyodYMQKiJu"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yXumtJ0oW5y",
        "outputId": "b8dffb00-19fd-478a-8a6f-344710189075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train: (29016, 16)\n",
            "t_train: (29016, 1)\n",
            "x_dev: (3627, 16)\n",
            "t_dev: (3627, 1)\n",
            "x_test: (3627, 16)\n",
            "t_test: (3627, 1)\n"
          ]
        }
      ],
      "source": [
        "# Modificación de las rutas en consecuencia para apuntar los archivos csv en su propio sistema de archivos gdrive..\n",
        "ATT_FILE = \"dataset/HotelReservationsPreparedCleanAttributes.csv\"\n",
        "LABEL_FILE = \"dataset/HotelReservationsOutput.csv\"\n",
        "\n",
        "# División del conjunto de datos en tres particiones: el 80% para el entrenamiento, el 10% para las pruebas de validación y el 10% restante para las pruebas de test.\n",
        "TRAIN_RATE = 0.8 # 80% del conjunto de datos se destina para el entrenamiento\n",
        "\n",
        "attributes = pd.read_csv(ATT_FILE)\n",
        "label = pd.read_csv(LABEL_FILE)\n",
        "\n",
        "#Se obtiene el numero de ejemplos dentro del conjunto de datos para cada una de las partes\n",
        "n_instances = attributes.shape[0]\n",
        "n_train = int(n_instances*TRAIN_RATE)\n",
        "n_dev = int((n_instances-n_train)/2) # 10% validation\n",
        "n_final_test = int (n_instances-n_train-n_dev) # 10% test\n",
        "\n",
        "#Se usa el numero de ejemplos para seleccionar cuantos se toman para cada parte\n",
        "#Desde el principio hasta n_train para entrenamiento\n",
        "x_train = attributes.values[:n_train]\n",
        "t_train = label.values[:n_train]\n",
        "\n",
        "#Desde donde acabo n_train hasta n_train + n_dev para validación\n",
        "x_dev = attributes.values[n_train:n_train+n_dev]\n",
        "t_dev = label.values[n_train:n_train+n_dev]\n",
        "\n",
        "#Desde donde acabó n_dev hasta el final\n",
        "x_test = attributes.values[n_train + n_dev:]\n",
        "t_test = label.values[n_train + n_dev:]\n",
        "\n",
        "#Imprimimos la forma de los conjuntos para comprobar que se ha hecho correctamente\n",
        "#Hay 36270 ejemplos, 16 atributos de entrada y 1 de salida\n",
        "# 36270 * 0.80 = 29016 ejemplos de entrenamiento\n",
        "print (\"x_train:\",x_train.shape)\n",
        "print (\"t_train:\",t_train.shape)\n",
        "\n",
        "# 36270 * 0.1 = 3627 ejemplos de validación/test\n",
        "print (\"x_dev:\",x_dev.shape)\n",
        "print (\"t_dev:\",t_dev.shape)\n",
        "\n",
        "print (\"x_test:\",x_test.shape)\n",
        "print (\"t_test:\",t_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUFgNlQF8nwQ"
      },
      "source": [
        "Se ha hecho una división de los datos 80% - 10% - 10% para un total de 36270 ejemplos en los datos.\n",
        "Esto equivale a:\n",
        "* 29016 ejemplos para entrenamiento\n",
        "* 3627 ejemplos para validación\n",
        "* 3627 ejemplos para test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlwBWRaeef-w"
      },
      "outputs": [],
      "source": [
        "# Balanceo de datos\n",
        "# !pip install imbalanced-learn\n",
        "# import imblearn\n",
        "# oversample = imblearn.over_sampling.RandomOverSampler(sampling_strategy='minority') # Balanceo aumentando la clase con menos muestras\n",
        "# x_train, t_train = oversample.fit_resample(x_train, t_train)\n",
        "# x_dev, t_dev = oversample.fit_resample(x_train, t_train)\n",
        "# t_train = t_train.reshape((len(t_train), 1))\n",
        "# t_dev = t_dev.reshape((len(t_train), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFxLit6PoW55"
      },
      "source": [
        "## Initialize variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "awoTBHr_oW56"
      },
      "outputs": [],
      "source": [
        "INPUTS = x_train.shape[1]\n",
        "OUTPUTS = t_train.shape[1]\n",
        "NUM_TRAINING_EXAMPLES = int(round(x_train.shape[0]/1))\n",
        "NUM_DEV_EXAMPLES = int (round (x_dev.shape[0]/1))\n",
        "NUM_TEST_EXAMPLES = int (round (x_test.shape[0]/1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dozYxaroW59"
      },
      "source": [
        "Se muestran algunos datos para comprobar que son correctos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo0AUb67oW6I",
        "outputId": "4c3f5a8e-8eda-43da-a30d-25114dfe948f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.        , -1.        , -0.71428571, -0.64705882, -1.        ,\n",
              "        -1.        ,  0.        , -1.        ,  0.27272727, -0.4       ,\n",
              "         0.        , -1.        , -1.        , -1.        , -0.54074074,\n",
              "        -1.        ],\n",
              "       [ 0.        , -1.        , -0.42857143, -0.64705882, -1.        ,\n",
              "        -1.        ,  0.        ,  0.32731377,  1.        ,  0.86666667,\n",
              "         0.        , -1.        , -1.        , -1.        , -0.69588889,\n",
              "        -0.6       ],\n",
              "       [-0.5       , -1.        , -0.42857143, -0.88235294,  0.        ,\n",
              "        -1.        ,  0.        , -0.4717833 ,  0.27272727, -1.        ,\n",
              "        -1.        , -1.        , -1.        , -1.        , -0.73148148,\n",
              "        -1.        ],\n",
              "       [-0.5       , -1.        , -1.        , -0.88235294, -1.        ,\n",
              "        -1.        ,  0.        , -0.98645598,  0.81818182,  0.53333333,\n",
              "         1.        ,  1.        , -0.84615385, -0.89655172, -0.75185185,\n",
              "        -0.6       ],\n",
              "       [ 0.        , -1.        , -1.        , -0.64705882, -1.        ,\n",
              "        -1.        ,  0.        , -0.62076749,  0.27272727, -0.66666667,\n",
              "         0.        , -1.        , -1.        , -1.        , -0.71666667,\n",
              "        -0.6       ]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nMjKqIVoW6K",
        "outputId": "1149e670-8505-42dc-ed7c-dfa2e70069e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RykFpOoboW6M",
        "outputId": "c8dbf554-49ab-4aa3-cbfe-36ef740858a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.5       , -1.        , -1.        , -0.88235294, -1.        ,\n",
              "        -1.        ,  0.        , -1.        ,  0.45454545, -0.46666667,\n",
              "         1.        , -1.        , -1.        , -1.        , -0.57777778,\n",
              "        -1.        ],\n",
              "       [ 0.        , -1.        , -1.        , -0.76470588, -1.        ,\n",
              "        -1.        ,  0.        ,  0.10609481, -0.09090909,  0.06666667,\n",
              "        -1.        , -1.        , -1.        , -1.        , -0.72222222,\n",
              "        -1.        ],\n",
              "       [ 0.        , -1.        , -0.42857143, -0.41176471, -1.        ,\n",
              "        -1.        ,  0.        , -0.87358916,  0.81818182, -0.93333333,\n",
              "        -1.        , -1.        , -1.        , -1.        , -0.84655556,\n",
              "        -1.        ],\n",
              "       [ 0.        , -1.        , -0.42857143, -0.64705882, -1.        ,\n",
              "        -1.        ,  0.        , -0.13318284,  0.45454545,  0.6       ,\n",
              "        -1.        , -1.        , -1.        , -1.        , -0.62222222,\n",
              "        -1.        ],\n",
              "       [ 0.        , -1.        , -0.71428571, -0.76470588,  0.        ,\n",
              "        -1.        ,  0.        , -0.52595937,  0.63636364, -0.26666667,\n",
              "        -1.        , -1.        , -1.        , -1.        , -0.59259259,\n",
              "        -1.        ]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_dev[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cy975RjoW6N",
        "outputId": "190d18b1-c91c-4a88-fe24-01dd78002881"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_dev[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgtbbfBIEHl8",
        "outputId": "ecb1fbaa-25f5-49f0-ead2-d86c473e5141"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.        , -1.        , -0.71428571, -0.76470588, -1.        ,\n",
              "        -1.        ,  0.        , -0.82844244, -0.27272727,  0.46666667,\n",
              "         0.        , -1.        , -1.        , -1.        , -0.53      ,\n",
              "        -0.6       ],\n",
              "       [-0.5       , -1.        , -1.        , -0.88235294, -1.        ,\n",
              "         1.        ,  0.        , -0.99097065, -0.27272727, -0.33333333,\n",
              "         1.        ,  1.        , -1.        , -0.75862069, -0.75185185,\n",
              "        -1.        ],\n",
              "       [ 0.        , -1.        , -0.71428571, -0.64705882, -1.        ,\n",
              "        -1.        ,  0.        , -0.76072235, -0.45454545, -0.33333333,\n",
              "         0.        , -1.        , -1.        , -1.        , -0.71637037,\n",
              "        -0.6       ],\n",
              "       [ 0.        , -1.        , -1.        , -0.76470588,  0.        ,\n",
              "        -1.        ,  0.        ,  0.16027088,  0.09090909, -1.        ,\n",
              "         0.        , -1.        , -1.        , -1.        , -0.62407407,\n",
              "        -1.        ],\n",
              "       [ 0.        , -1.        , -0.71428571, -0.64705882, -1.        ,\n",
              "        -1.        ,  0.        ,  0.10609481,  1.        ,  0.86666667,\n",
              "         0.        , -1.        , -1.        , -1.        , -0.46      ,\n",
              "        -0.6       ]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh9G4pAzEHl9",
        "outputId": "1297380a-92ca-4bbb-9c6f-0023b670379c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t_test[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1N4Lt6QoW6P"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkkRVc5doW6Q"
      },
      "source": [
        "La tasa de aprendizaje $\\alpha$, el tamaño del batch y el número de neuronas en la/las capas ocultas son los tres hiperparámetros a ajustar. El proceso de aprendizaje se detiene cuando alcanza el número de epochs indicado.\n",
        "\n",
        "Recordemos que un **epoch** significa presentar todo el conjunto de datos de entrenamiento a la red neuronal.\n",
        "\n",
        "Adoptamos un descenso de gradiente por mini lotes como estrategia de actualización de parámetros con un tamaño de mini lotes de *batch_size*. Dado que el tamaño del conjunto de datos de entrenamiento es *P=29.016*, una época consta de *P/batch_size* iteraciones de aprendizaje por epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nuHkCgWaoW6Q"
      },
      "outputs": [],
      "source": [
        "activation_functions = ['tanh', 'relu']\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "batch_size= 256\n",
        "layers = [3, 5, 7]\n",
        "hidden = [8, 12, 16, 32, 64]\n",
        "dropouts = [0.1, 0.2, 0.3]\n",
        "n_epochs = 600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aeLUFO3k_Eoz"
      },
      "outputs": [],
      "source": [
        "# Add batch normalization\n",
        "def create_model(activation_function, learning_rate, batch_size, n_layers, n_hidden, dropout):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        model.add(Dense(n_hidden, activation=activation_function))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t-6ki6eeMBNI"
      },
      "outputs": [],
      "source": [
        "class MyHyperModel(keras_tuner.HyperModel):\n",
        "\n",
        "    def build(self, hp):\n",
        "        \"\"\"Creates a Keras model instance.\"\"\"\n",
        "        activation_fn = hp.Choice('activation_function', activation_functions)\n",
        "        learning_rate = hp.Choice('learning_rate', learning_rates)\n",
        "        batch = batch_size\n",
        "        n_layers = hp.Choice('n_layers', layers)\n",
        "        n_hidden = hp.Choice('n_hidden', hidden)\n",
        "        dropout = hp.Choice('dropout', dropouts)\n",
        "\n",
        "        model = create_model(activation_fn, learning_rate, batch, n_layers, n_hidden, dropout)\n",
        "\n",
        "        # Define the learning rate schedule\n",
        "        def lr_schedule(epoch, lr):\n",
        "            if epoch < 10:\n",
        "                return lr\n",
        "            else:\n",
        "                return lr * tf.math.exp(-0.1)\n",
        "\n",
        "        # Use SGD optimizer with learning rate schedule\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5)])\n",
        "\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I9lZemrIEhfy"
      },
      "outputs": [],
      "source": [
        "class History(Callback):\n",
        "    def __init__(self):\n",
        "        super(History, self).__init__()\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.accuracies = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.accuracies.append(logs.get('accuracy'))\n",
        "        self.val_accuracies.append(logs.get('val_accuracy'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL4ErYRvODX1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew16gBuN9-p-"
      },
      "source": [
        "Busqueda Bayesiana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PozItZq3997w",
        "outputId": "43ca5337-d9b7-4a12-9f09-34cadef03451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 417 Complete [00h 01m 07s]\n",
            "val_binary_accuracy: 0.8698648810386658\n",
            "\n",
            "Best val_binary_accuracy So Far: 0.8737248182296753\n",
            "Total elapsed time: 08h 48m 05s\n",
            "\n",
            "Search: Running Trial #418\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "tanh              |tanh              |activation_function\n",
            "0.01              |0.01              |learning_rate\n",
            "3                 |3                 |n_layers\n",
            "64                |64                |n_hidden\n",
            "0.1               |0.1               |dropout\n",
            "\n",
            "Epoch 1/600\n"
          ]
        }
      ],
      "source": [
        "tuner = keras_tuner.BayesianOptimization(\n",
        "    hypermodel=MyHyperModel(),\n",
        "    objective=keras_tuner.Objective('val_binary_accuracy', direction=\"max\"),\n",
        "    max_trials=1000,\n",
        "    overwrite=True,\n",
        ")\n",
        "history = History()\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=5), history]\n",
        "\n",
        "tuner.search(x_train, t_train, epochs=n_epochs, batch_size=batch_size, validation_data=(x_dev, t_dev), callbacks=callbacks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWlilGdd_8En"
      },
      "outputs": [],
      "source": [
        "# Get the best model\n",
        "best_hps = tuner.get_best_hyperparameters()\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Evaluate the best model\n",
        "best_model.evaluate(x_test, t_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRLDF1BgoW6R"
      },
      "source": [
        "## Build the model\n",
        "Una arquitectura de red neuronal 16 - capas ocultas - 1 totalmente conectada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xxIaRxUk3M4"
      },
      "outputs": [],
      "source": [
        "model = best_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_VmxXz7oW6T"
      },
      "source": [
        "Definición de la topología de la red neuronal profunda. Se tiene en cuenta que la función de activación **tanh** se elige para la capa oculta debido a que los valores de nuestros datos están comprendidos en el rango (-1,1). Por otra parte se usa **sigmoid** para la capa de salida porque esta función alcanza sus límites en 0 y 1, esto nos conviene porque la salida esperada de la red debe ser 0 o 1 indicando si se cancela la reserva del hotel o no se cancela respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWat5nSv4iWe"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEsu9_TpoW6U"
      },
      "source": [
        "There are 13,003 parameters to adjust in the learning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nGUW64BmNeH"
      },
      "source": [
        "## Compile the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XncgX3DKmU7m"
      },
      "source": [
        "Compilar el modelo significa especificar:\n",
        "* La función de *pérdida*, la $entropía cruzada$ en este caso ya que nuestro problema es de clasificación y más especificamente clasificación binaria. Esta función mide la discrepancia entre las predicciones y las etiquetas reales en un problema de clasificación.\n",
        "* El *optimizador* Gradient Descent a utilizar, dado que se ha estudiado en clase.\n",
        "* Una lista de *métricas* adicionales (Precisión) a calcular durante el entrenamiento y la evaluación, en este caso se usa la precisión binaria dado que calcula la frecuencia con la que las predicciones coinciden con las etiquetas binarias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXcGF4pSm0VH"
      },
      "source": [
        "There are several loss functions, optimizers, and metrics. Full lists are available at: https://keras.io/losses/, https://keras.io/optimizers/ and https://keras.io/metrics/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf4nKA8DoW6W"
      },
      "outputs": [],
      "source": [
        "# # Crear un objeto tqdm para el bucle de entrenamiento\n",
        "# progress_bar = tqdm(total=n_epochs, desc='Training Progress', position=0)\n",
        "\n",
        "# start = time.perf_counter()\n",
        "# history = model.fit(x_train, t_train, batch_size=batch_size, epochs=n_epochs, verbose=0, validation_data=(x_dev, t_dev), callbacks=[LambdaCallback(on_epoch_end=lambda epoch, logs: progress_bar.update(1))])\n",
        "# progress_bar.close()\n",
        "# print(time.perf_counter() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2FPDNVEn0-F"
      },
      "source": [
        "El entrenamiento es más rápido con verbose=0 que con verbose=1, pero este último nos permite depurar el código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sdXmmqFoW6Y"
      },
      "source": [
        "## Get the results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVKp4GProW6Y"
      },
      "outputs": [],
      "source": [
        "# Access the history from the tuner\n",
        "# Use the losses and accuracies from the History object\n",
        "loss_data = pd.DataFrame(history.losses, columns=['loss'])\n",
        "val_loss_data = pd.DataFrame(history.val_losses, columns=['val_loss'])\n",
        "acc_data = pd.DataFrame(history.accuracies, columns=['accuracy'])\n",
        "val_acc_data = pd.DataFrame(history.val_accuracies, columns=['val_accuracy'])\n",
        "\n",
        "# Check if there are any NaN values\n",
        "if loss_data.isna().any().any() or val_loss_data.isna().any().any() or acc_data.isna().any().any() or val_acc_data.isna().any().any():\n",
        "    print(\"Warning: There are NaN values in the DataFrame.\")\n",
        "\n",
        "# Plot the results\n",
        "results = pd.concat([loss_data, val_loss_data, acc_data, val_acc_data], axis=1)\n",
        "results.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy - Mean Log Loss\")\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iloQRzMTob48"
      },
      "outputs": [],
      "source": [
        "results[-1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lng_ym78dlwi"
      },
      "source": [
        "Veamos cómo predice el modelo en el conjunto de validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfoo4xH1bnks"
      },
      "outputs": [],
      "source": [
        "dev_predictions=model.predict(x_dev).round(2)\n",
        "dev_predictions[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caEEbY6lb0eK"
      },
      "outputs": [],
      "source": [
        "dev_rounded_predictions=np.round(dev_predictions).astype(int)\n",
        "dev_rounded_predictions[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52L6dX94b335"
      },
      "outputs": [],
      "source": [
        "t_dev[:20] #target classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qGHRMG_b75T"
      },
      "outputs": [],
      "source": [
        "#dev_correct_predictions = np.equal(np.argmax(dev_rounded_predictions,1),np.argmax(t_dev,1))\n",
        "#print (dev_correct_predictions[:30])\n",
        "\n",
        "dev_correct_predictions = (dev_rounded_predictions == t_dev)\n",
        "print(dev_correct_predictions[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1ReCnKAb_7b"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "# Aplanar el array 2D a una lista de booleanos\n",
        "dev_correct_predictions_flat = dev_correct_predictions.flatten()\n",
        "#print(dev_correct_predictions_flat[:20])\n",
        "\n",
        "# Convertir el array a una lista de enteros\n",
        "dev_correct_predictions_list = list(dev_correct_predictions_flat.astype(int))\n",
        "#print(dev_correct_predictions_list[:20])\n",
        "\n",
        "# Usar Counter con la lista\n",
        "counter_result = Counter(dev_correct_predictions_list)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(counter_result)\n",
        "#1 = la cantidad de Trues, es decir, la cantidad de coincidencias entre la prediccion y el valor esperado\n",
        "#0 = la cantidad de False, es decir, la cantidad de discrepancias entre la prediccion y el valor esperado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMsV4LjdxhxQ"
      },
      "source": [
        "####Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQbaO0BnxlJI"
      },
      "outputs": [],
      "source": [
        "# Obtener las predicciones y etiquetas verdaderas como arrays de 0 y 1\n",
        "dev_binary_predictions = np.round(dev_predictions).flatten()\n",
        "dev_binary_targets = t_dev.flatten()\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(dev_binary_targets, dev_binary_predictions)\n",
        "\n",
        "# Crear un DataFrame para visualizar la matriz de confusión\n",
        "c_m = pd.DataFrame(conf_matrix, columns=['True_Negative', 'True_Positive'])\n",
        "c_m.insert(0, 'Classes', ['Pred_Negative', 'Pred_Positive'])\n",
        "c_m['Sum'] = c_m.sum(axis=1, numeric_only=True)\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(c_m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHHqL9cyN6wh"
      },
      "source": [
        "####Clasification metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6ZXnks5yar3"
      },
      "source": [
        "**Precisión** es la proporción de observaciones correctamente predichas en una clase respecto al total de observaciones predichas para esa clase (fila). La pregunta que responde esta métrica es: de todas las predicciones etiquetadas como una especie, ¿cuántas realmente pertenecen a esa clase?\n",
        "\n",
        "$Precision = \\frac{TP}{TP+FP}$\n",
        "\n",
        "**Recall** es la proporción de observaciones correctamente predichas respecto a todas las observaciones en la clase real (columna). La pregunta que responde recall es: de todas las observaciones que realmente pertenecen a una clase, ¿cuántas etiquetó correctamente el modelo? Por lo general, el recall se refiere a la clase positiva, también conocida como *sensibilidad*, mientras que el recall de la clase negativa se denomina *especificidad*.\n",
        "\n",
        "Recall (sensitivity) $= \\frac{TP}{TP+FN}$\n",
        "\n",
        "Specificity $= \\frac{TN}{TN+FP}$\n",
        "\n",
        "**F1 score** es el promedio ponderado de Precisión y Recall. Por lo tanto, este puntaje tiene en cuenta ambas clases de errores. Intuitivamente, no es tan fácil de entender como la precisión, pero el F1 suele ser más útil que la precisión, especialmente si tienes una distribución desigual de clases. La exactitud funciona mejor si la precisión y el recall tienen costos similares. Si el costo de los falsos positivos y falsos negativos es muy diferente, es mejor considerar tanto la precisión como el recall.\n",
        "\n",
        "f1-score $= \\frac{2(Recall * Precision)} {Recall + Precision}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hVgDIU4ymaz"
      },
      "outputs": [],
      "source": [
        "print('Classification Report')\n",
        "print(classification_report(dev_binary_targets, dev_binary_predictions, target_names=['Class_0', 'Class_1']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtmMDtloEHmD"
      },
      "outputs": [],
      "source": [
        "# COMPROBAR EL MODELO CON LOS DATOS DE TEST\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "# Obtener las predicciones del modelo en el conjunto de test\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Redondear las predicciones para un problema de clasificación binaria\n",
        "rounded_predictions = np.round(predictions)\n",
        "\n",
        "# Calcular métricas de evaluación\n",
        "accuracy = accuracy_score(t_test, rounded_predictions)\n",
        "precision = precision_score(t_test, rounded_predictions)\n",
        "recall = recall_score(t_test, rounded_predictions)\n",
        "f1 = f1_score(t_test, rounded_predictions)\n",
        "\n",
        "# Imprimir o utilizar las métricas según sea necesario\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1}')\n",
        "\n",
        "# También puedes imprimir la matriz de confusión\n",
        "conf_matrix = confusion_matrix(t_test, rounded_predictions)\n",
        "# Crear un DataFrame de Pandas con nombres de filas y columnas\n",
        "confusion_df = pd.DataFrame(conf_matrix, index=['Negativo', 'Positivo'], columns=['Pred_Negativo', 'Pred_Positivo'])\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4036622,
          "sourceId": 7019940,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30587,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
